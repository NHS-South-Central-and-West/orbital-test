{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9f3c826",
   "metadata": {},
   "source": [
    "# Demonstration of Posit Orbital\n",
    "\n",
    "[Introduction to Orbital](https://posit.co/blog/introducing-orbital-for-scikit-learn-pipelines/)\n",
    "\n",
    "[Orbital Repository](https://github.com/posit-dev/orbital)\n",
    "\n",
    "We are going to do a Logistic Regression model. The data is what was used at our Frequent Attenders Hackathon, and the processing, pipeline and model are based on what Paul J has done for his [speedrun demo](https://github.com/NHS-South-Central-and-West/frequent-attenders/tree/main) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e65337",
   "metadata": {},
   "source": [
    "## Set up our warehouse connection\n",
    "\n",
    "This is getting the data we used for the hackathon directly from the data warehouse.\n",
    "\n",
    "**Note:** the `params.py` has been .gitignored so that the confidential connection details don't get published online."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9e6f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from utils.warehouse_connection import warehouse_connection\n",
    "from utils import params\n",
    "\n",
    "df = warehouse_connection(f'SELECT * FROM {params.TABLE_NAME}')\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78595de3",
   "metadata": {},
   "source": [
    "## Create train and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4249eee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.split_data import tt_split\n",
    "\n",
    "train_df, test_df = tt_split(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9371a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4d597c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be02a77d",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b006af25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e746da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = ['Organisation_Code_Provider', 'Age_At_Arrival',\n",
    "       'Index_Of_Multiple_Deprivation', 'Stated_Gender',\n",
    "       'Arrival_Mode_Desc', 'Attendance_Category',\n",
    "       'Long_Term_Condition_Count_Number', 'GP_Practice_Code',\n",
    "       'Care_Home_Status', \n",
    "       'Living_Alone',\n",
    "       'Disability_Count_Number',\n",
    "       'Segmentation_Bridges_To_Health',]\n",
    "\n",
    "target = ['frequent_attender']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1248e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "feats + target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95aeaf72",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.dropna(subset=feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01568b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['Index_Of_Multiple_Deprivation'] = train_df['Index_Of_Multiple_Deprivation'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62bdb7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_df[feats]\n",
    "y = train_df[target]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33fde3dd",
   "metadata": {},
   "source": [
    "## Create the model pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd96ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "cat_feats = ['Organisation_Code_Provider', 'Stated_Gender', 'Arrival_Mode_Desc', \n",
    "            'GP_Practice_Code', 'Living_Alone', 'Attendance_Category', 'Care_Home_Status']\n",
    "num_feats = ['Age_At_Arrival', 'Index_Of_Multiple_Deprivation', 'Long_Term_Condition_Count_Number', \n",
    "            'Segmentation_Bridges_To_Health', 'Disability_Count_Number']\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[(\"encoder\", OneHotEncoder(handle_unknown='ignore'))])\n",
    "numeric_transformer = Pipeline(steps=[(\"scaler\", StandardScaler())])\n",
    "\n",
    "col_transformer = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, num_feats),\n",
    "        (\"cat\", categorical_transformer, cat_feats),\n",
    "    ]\n",
    ")\n",
    "\n",
    "preprocessor = Pipeline(steps=[(\"col_transformer\", col_transformer)])\n",
    "\n",
    "clf = Pipeline(\n",
    "    steps=[(\"preprocessor\", preprocessor), \n",
    "           (\"regressor\", LogisticRegression(max_iter=1000))]\n",
    ")\n",
    "\n",
    "clf.fit(X_train, np.ravel(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d2fd00",
   "metadata": {},
   "source": [
    "## Create an Orbital pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847375cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import orbital"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b0062f",
   "metadata": {},
   "source": [
    "We need the X column names and the data types to go into the Orbital pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b6b25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30755f1a",
   "metadata": {},
   "source": [
    "Would be nice to have a way to do this without having to write them all out again, but `orbital.types.guess_datatypes(X)` didn't work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7c5489",
   "metadata": {},
   "outputs": [],
   "source": [
    "orbital_pipeline = orbital.parse_pipeline(\n",
    "    pipeline=clf,\n",
    "    features={\n",
    "        'Organisation_Code_Provider': orbital.types.StringColumnType(),\n",
    "        'Age_At_Arrival': orbital.types.Int64ColumnType(),\n",
    "        'Index_Of_Multiple_Deprivation': orbital.types.Int64ColumnType(),\n",
    "        'Stated_Gender': orbital.types.StringColumnType(),\n",
    "        'Arrival_Mode_Desc': orbital.types.StringColumnType(),\n",
    "        'Attendance_Category': orbital.types.StringColumnType(),\n",
    "        'Long_Term_Condition_Count_Number': orbital.types.FloatColumnType(),\n",
    "        'GP_Practice_Code': orbital.types.StringColumnType(),\n",
    "        'Care_Home_Status': orbital.types.StringColumnType(),\n",
    "        'Living_Alone': orbital.types.StringColumnType(),\n",
    "        'Disability_Count_Number': orbital.types.FloatColumnType(),\n",
    "        'Segmentation_Bridges_To_Health': orbital.types.FloatColumnType(),\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8429f48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(orbital_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748a938e",
   "metadata": {},
   "source": [
    "## Generate SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d038c9c9",
   "metadata": {},
   "source": [
    "Get the list of SQL dialects available, since this is a required argument for generating the SQL. Bung it into a DataFrame to make it more easily browsable with Data Wrangler.\n",
    "\n",
    "Ours is listed as \"TSQL\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3999fb9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlglot\n",
    "dialects = pd.DataFrame(sqlglot.dialects.DIALECTS)\n",
    "dialects\n",
    "\n",
    "# if you don't have Data Wrangler, it's the last one, so run dialects.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c0ad2e",
   "metadata": {},
   "source": [
    "Having said that, I tried \"TSQL\" and it didn't work, returning an Ibis-related error. Looking at the [docs](https://ibis-project.org/backends/mssql), the dialect of SQL we need is \"mssql\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f195bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import params\n",
    "\n",
    "sql = orbital.export_sql(\n",
    "    table_name=params.JUST_THE_TABLE,  # sqlglot just wants the table name as a string, not [d].[s].[t] format.\n",
    "    pipeline=orbital_pipeline,\n",
    "    dialect='mssql',\n",
    "    optimize=False # this prevents sqlglot from trying to produce aliases for the pipeline, which trips it up when using mssql.\n",
    ")\n",
    "\n",
    "print(sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e5f34c",
   "metadata": {},
   "source": [
    "### Making the MSSQL produced by Orbital compatible with our 2016 SQL Server.\n",
    "\n",
    "Orbital produces SQL that uses functions that have been introduced more recently than 2016, so some of the functions are not available. For example, Orbital uses the function `GREATEST`, which only became supported by SQL Server from 2022."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbdd1bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.sql_compatibility import adapt_sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d878184",
   "metadata": {},
   "outputs": [],
   "source": [
    "adapted_sql = adapt_sql(sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5737b9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(adapted_sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c791b294",
   "metadata": {},
   "source": [
    "Let's try running the adapted SQL. On the first attempt, this took about two minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9281d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = warehouse_connection(adapted_sql)\n",
    "\n",
    "results.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "orbital-test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
